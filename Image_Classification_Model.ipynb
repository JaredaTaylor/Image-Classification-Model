{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf798513",
      "metadata": {
        "id": "cf798513"
      },
      "source": [
        "# CISC/CMPE 452/COGS 400 Group Project - Distracted Driver Detection\n",
        "\n",
        "Please put your name and student id\n",
        "\n",
        "    Jared Taylor, 20075820\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VRLNrFDU3dKp",
      "metadata": {
        "id": "VRLNrFDU3dKp"
      },
      "source": [
        "1. Create model\n",
        "2. Train model\n",
        "3. Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e6a263",
      "metadata": {
        "id": "b7e6a263"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c947ce18",
      "metadata": {},
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d33c072",
      "metadata": {},
      "outputs": [],
      "source": [
        "#preprocessing class\n",
        "class PreProcessing:\n",
        "  '''\n",
        "  Goes through all images, returns preprocessed tensor.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, base_path):\n",
        "      self.base_path = base_path\n",
        "      self.kernel = np.array([[-1, -1, -1],\n",
        "                   [-1, 8,-1],\n",
        "                   [-1, -1, -1]])\n",
        "      print(self.base_path)\n",
        "\n",
        "  def get_colour_type(self, img_path):\n",
        "    image = cv2.imread(img_path)\n",
        "    if len(image.shape) == 3: return 3\n",
        "    else: return 1\n",
        "\n",
        "  def preprocess_image(self, img_path, height, width):\n",
        "    '''\n",
        "    Function takes the path to the image and applys the preprocessing.\n",
        "    '''\n",
        "\n",
        "    color_type = self.get_colour_type(img_path)\n",
        "\n",
        "    if color_type == 1:\n",
        "        img = cv2.imread(img_path, 0)\n",
        "        img_gray = cv2.threshold(img,0,255,cv2.THRESH_TRUNC+cv2.THRESH_OTSU) \n",
        "        image_sharp = cv2.filter2D(src=img, ddepth=-1, kernel=self.kernel)\n",
        "\n",
        "    elif color_type == 3:\n",
        "        img = cv2.imread(img_path)\n",
        "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        img_gray = cv2.threshold(img_gray,0,255,cv2.THRESH_TRUNC+cv2.THRESH_OTSU)\n",
        "        image_sharp = cv2.filter2D(src=img, ddepth=-1, kernel=self.kernel)\n",
        "        image_sharp = cv2.cvtColor(image_sharp, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "\n",
        "    combined = cv2.add(image_sharp, img_gray[1])\n",
        "    dst = cv2.resize(combined, (width, height))\n",
        "    dst = cv2.cvtColor(dst, cv2.COLOR_GRAY2BGR)\n",
        "    \n",
        "    return dst\n",
        "\n",
        "  def get_driver_data(self):\n",
        "    '''\n",
        "    Returns a dictionary of image name as the key and driver and class as value.\n",
        "    '''\n",
        "\n",
        "    driver_data = {}\n",
        "    path = os.path.join(self.base_path,'driver_imgs_list.csv')\n",
        "\n",
        "    print('Read drivers data')\n",
        "\n",
        "    with open(path, 'r') as file:\n",
        "      lines = file.readlines()\n",
        "      lines = lines[1:]\n",
        "    file.close()\n",
        "\n",
        "    for line in lines:\n",
        "      arr = line.strip().split(',')\n",
        "      driver_data[arr[2]] = (arr[0], arr[1])\n",
        "    \n",
        "    return driver_data\n",
        "\n",
        "  def load_train_data(self, height, width):\n",
        "    '''\n",
        "    loads driver data\n",
        "    '''\n",
        "\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    driver_ids = []\n",
        "\n",
        "    driver_data = self.get_driver_data()\n",
        "\n",
        "    print('Read train images')\n",
        "    for class_number in range(10):\n",
        "        print(f'Load folder c{class_number}')\n",
        "        class_number_str = 'c' + str(class_number)\n",
        "        path = os.path.join(self.base_path, 'imgs/train', class_number_str, '*.jpg')\n",
        "        file_paths = glob.glob(path)  # Gets all file names matching given path.\n",
        "        for file_path in file_paths:\n",
        "            file_name = os.path.basename(file_path)\n",
        "            image = self.preprocess_image(file_path, height, width)\n",
        "            x_train.append(image)\n",
        "            y_train.append(class_number)\n",
        "            driver_id = driver_data[file_name][0]\n",
        "            driver_ids.append(driver_id)\n",
        "\n",
        "    return x_train, y_train, driver_ids\n",
        "\n",
        "  # Not used since data has no classification\n",
        "  def load_test_data(self, height, width):\n",
        "    x_test = []\n",
        "    x_test_ids = []\n",
        "    \n",
        "    print('Read test images')\n",
        "\n",
        "    path = os.path.join(self.base_path, 'imgs/test/*.jpg')\n",
        "    file_paths = glob.glob(path)\n",
        "    number_of_files = len(file_paths)\n",
        "\n",
        "    for count, file_path in enumerate(file_paths):\n",
        "        file_name = os.path.basename(file_path)\n",
        "        image = self.preprocess_image(file_path, height, width)\n",
        "        x_test.append(image)\n",
        "        x_test_ids.append(file_name)\n",
        "        if count % 1000 == 0:\n",
        "            print(f\"Read {count} images from {number_of_files}\")\n",
        "\n",
        "    return x_test, x_test_ids\n",
        "\n",
        "\n",
        "  def split_train_data_on_class(self, x_train, y_train):\n",
        "    '''\n",
        "    split training data into new train and test sets, based off percentage in each classification\n",
        "    '''\n",
        "\n",
        "    newTrain_x = []\n",
        "    newTrain_y = []\n",
        "    newTest_x = []\n",
        "    newTest_y = []\n",
        "    c = [[],[],[],[],[],[],[],[],[],[]] #[c0, c1, c2, c3, c4, c5, c6, c7, c8, c9]\n",
        "    #fill classified train data\n",
        "    for ind in range(len(y_train)):\n",
        "      cls = y_train[ind]\n",
        "      c[cls].append(x_train[ind])\n",
        "    #for each classification, split 75% train, 25% test\n",
        "    currentClass = 0\n",
        "    for cls in c:\n",
        "      splitPoint = int((len(cls) * 0.75) // 1)\n",
        "      for ind in range(len(cls)):\n",
        "        if ind <= splitPoint:\n",
        "          newTrain_x.append(cls[ind])\n",
        "          newTrain_y.append(currentClass)\n",
        "        else:\n",
        "          newTest_x.append(cls[ind])\n",
        "          newTest_y.append(currentClass)\n",
        "      currentClass += 1\n",
        "    return np.array(newTrain_x), np.array(newTrain_y), np.array(newTest_x), np.array(newTest_y)\n",
        "\n",
        "\n",
        "  def split_train_data_on_driver(self, x_train, y_train, driver_ids):\n",
        "    '''\n",
        "    divide train data into new test and train based on driver ids\n",
        "    '''\n",
        "\n",
        "    idList = []\n",
        "    newTrain_x = []\n",
        "    newTrain_y = []\n",
        "    newTest_x = []\n",
        "    newTest_y = []\n",
        "    for driver in driver_ids:\n",
        "        if driver not in idList:\n",
        "            idList.append(driver)\n",
        "    trainData = idList[:20]\n",
        "    testData = idList[20:]\n",
        "    #iterate trough x_train, compare driver_ids\n",
        "    for ind in range(len(x_train)):\n",
        "        if driver_ids[ind] in trainData:\n",
        "            newTrain_x.append(x_train[ind])\n",
        "            newTrain_y.append(y_train[ind])\n",
        "        if driver_ids[ind] in testData:\n",
        "            newTest_x.append(x_train[ind])\n",
        "            newTest_y.append(y_train[ind])\n",
        "    return np.array(newTrain_x), np.array(newTrain_y), np.array(newTest_x), np.array(newTest_y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f7144b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def shuffle_data(x, y):\n",
        "    '''\n",
        "    shuffle x and y of data set\n",
        "    '''\n",
        "    \n",
        "    # x and y are same length\n",
        "    temp = []\n",
        "    for i in range(len(x)):\n",
        "        temp.append((x[i], y[i]))\n",
        "    np.random.shuffle(temp)\n",
        "    t_x = []\n",
        "    t_y = []\n",
        "    for j in range(len(temp)):\n",
        "        t_x.append(temp[j][0])\n",
        "        t_y.append(temp[j][1])\n",
        "    x = t_x\n",
        "    y = t_y\n",
        "    return np.array(x), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a6c63f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get processed data\n",
        "PATH = 'data'\n",
        "p = PreProcessing(PATH)\n",
        "x_data, y_data, driver_ids = p.load_train_data(112, 112)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726fd52f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# split train data and shuffle training set\n",
        "x_train, y_train, x_test, y_test = p.split_train_data_on_class(x_data, y_data)\n",
        "x_train, y_train = shuffle_data(x_train, y_train)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07a22831",
      "metadata": {},
      "outputs": [],
      "source": [
        "# data visualization\n",
        "temp = x_test[0]\n",
        "cv2.imshow('image', temp)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99958410",
      "metadata": {},
      "source": [
        "# Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a44080",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ICV3:\n",
        "    '''\n",
        "    creates model\n",
        "    '''\n",
        "\n",
        "    def __init__(self, width, height):\n",
        "        inputShape = (width, height, 3)\n",
        "        initLearningRate = 0.01\n",
        "        self.lrSchedule = keras.optimizers.schedules.ExponentialDecay(initLearningRate, decay_steps = 526, decay_rate = 0.95, staircase = True)\n",
        "        temp = keras.callbacks.ModelCheckpoint\n",
        "        self.ICV3 = self.buildModel(inputShape)\n",
        "        self.ICV3.summary()\n",
        "    \n",
        "\n",
        "    def fit(self, numEpochs, x_train, y_train, batchSize):\n",
        "        self.ICV3.fit(x_train, y_train, epochs = numEpochs, batch_size = batchSize, verbose = 1)\n",
        "\n",
        "\n",
        "    def buildModel(self, inputShape):\n",
        "        preTrainedModel = InceptionV3(input_shape = inputShape, include_top = False, weights = None)\n",
        "        WEIGHTS_PATH_NO_TOP = (\n",
        "            'https://github.com/fchollet/deep-learning-models/'\n",
        "            'releases/download/v0.5/'\n",
        "            'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "        iv3Weights = keras.utils.get_file('inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5', WEIGHTS_PATH_NO_TOP, cache_subdir='models',\n",
        "                     file_hash='bcbd6486424b2319ff4ef7d526e38f63')\n",
        "        preTrainedModel.load_weights(iv3Weights)\n",
        "        for layer in preTrainedModel.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        lastLayer = preTrainedModel.get_layer('mixed7')\n",
        "        lastOutput = lastLayer.output\n",
        "\n",
        "        x = keras.layers.Flatten()(lastOutput)\n",
        "        x = keras.layers.Dense(1024, activation='relu')(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "        x = keras.layers.Flatten()(x)\n",
        "        x = keras.layers.Dense(512, activation='relu')(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.Dropout(0.2)(x)\n",
        "        x = keras.layers.Dense(10, activation = 'softmax')(x)\n",
        "\n",
        "        model = keras.Model(preTrainedModel.input, x)\n",
        "        model.compile(optimizer = keras.optimizers.Adam(learning_rate = self.lrSchedule), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "        return model\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea34d315",
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 20\n",
        "batchSize = 32\n",
        "model1 = ICV3(112, 112)\n",
        "model1.fit(epochs, x_train, y_train, batchSize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad888863",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy and Scores\n",
        "test_loss, test_acc = model1.ICV3.evaluate(x_test, y_test)\n",
        "print(f'\\nTest lost: {test_loss} -- Test accuracy: {test_acc}')\n",
        "y_pred = model1.ICV3.predict(x_test, batch_size = 32, verbose = 1)\n",
        "\n",
        "predicted = np.argmax(y_pred, axis=1)\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "print('Confusion matrix:')\n",
        "for row in cm:\n",
        "    print(row)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "A1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
